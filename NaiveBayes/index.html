<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Naive Bayes Classifier</title>
    <meta name="author" content="name" />
    <meta name="description" content="description here" />
    <meta name="keywords" content="keywords,here" />
    <link rel="stylesheet" href="https://unpkg.com/tailwindcss@2.2.19/dist/tailwind.min.css"/>
    <!--Replace with your tailwind.css once created-->
	<style>
		
		code { 
  			font-family: monospace;
			}
		img {
  			display: block;
 			margin-left: auto;
  			margin-right: auto;
			}
		</style>
  </head>

  
<body class="bg-gray-100 font-sans leading-normal tracking-normal">

	<nav id="header" class="fixed w-full z-10 top-0">

		<div id="progress" class="h-1 z-20 top-0" style="background:linear-gradient(to right, #4dc0b5 var(--scroll), transparent 0);"></div>

		<div class="w-full md:max-w-4xl mx-auto flex flex-wrap items-center justify-between mt-0 py-3">

			<div class="pl-4">
				<a class="text-gray-900 text-base no-underline hover:no-underline font-extrabold text-xl" href="#">
					Naive Bayes Classifier
				</a>
			</div>

			<div class="block lg:hidden pr-4">
				<button id="nav-toggle" class="flex items-center px-3 py-2 border rounded text-gray-500 border-gray-600 hover:text-gray-900 hover:border-green-500 appearance-none focus:outline-none">
					<svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
						<title>Menu</title>
						<path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" />
					</svg>
				</button>
			</div>

			<div class="w-full flex-grow lg:flex lg:items-center lg:w-auto hidden lg:block mt-2 lg:mt-0 bg-gray-100 md:bg-transparent z-20" id="nav-content">
				<ul class="list-reset lg:flex justify-end flex-1 items-center">
					<li class="mr-3">
						<a target="_blank" class="inline-block py-2 px-4 text-gray-900 font-bold no-underline" href="#">Active</a>
					</li>
					<li class="mr-3">
						<a target="_blank" class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-2 px-4" href="https://www.linkedin.com/in/sakshi-srivastava-aa2a5591/">linkedin</a>
					</li>
					<li class="mr-3">
						<a target="_blank" class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-2 px-4" href="https://github.com/sakshi0303/">Github</a>
					</li>
				</ul>
			</div>
		</div>
	</nav>

	<!--Container-->
	<div class="container w-full md:max-w-3xl mx-auto pt-20">
		<div class="w-full px-4 md:px-6 text-xl text-gray-800 leading-normal" style="font-family:Georgia,serif;">
			<!--Title-->
			<div class="font-sans">
				<p class="text-base md:text-sm text-green-500 font-bold"><!--- <a href="#" class="text-base md:text-sm text-green-500 font-bold no-underline hover:underline">BACK TO BLOG</a> --></p>
				<!-- <blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
				<h1 class="font-bold font-sans break-normal text-gray-900 pt-6 pb-2 text-3xl md:text-4xl"> Image Classifier</h1></blockquote> -->
						<p class="text-sm md:text-base font-normal text-gray-600">Published 14 November 2022</p> 
			</div>
			<!--Post Content-->
			<!--Lead Para-->
			<p class="py-6">
				üëã Hi!!
				Naive Bayes Classifications is a Supervised machine learning algorithm used for classification.It consist of two words Naive and Bayes. So let‚Äôs first discuss the Bayes Theorem.
				
				<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
					<b>What is Bayes Theorem?</b>
				</blockquote>
				
				Bayes theorem works on conditional probability. Conditional probability is the probability that something will happen, given that something else has already occurred. Using the conditional probability, we can calculate the probability of an event using its prior knowledge.
				It is stated mathematically as the following equation,
				<a target="_blank" class="text-green-500 no-underline hover:underline" href="https://www.kaggle.com/datasets/gaveshjain/ford-sentence-classifiaction-dataset?resource=download">DataSet from Kaggle</a><br/>
				<img src="images/naivebayesformula.jpg" alt="" >
				<br/>
				where,<br/>
    					<b>‚Äì</b> A and B are events and P(B)!=0
    					<br/><b>‚Äì</b> P(A | B) is a conditional probability that defines the likelihood of event A occurring given that B is true, also ‚Äì known as posterior probability,
    					<br/><b>‚Äì</b> P(B | A) is a conditional probability that defines the likelihood of event B occurring given that A is true, also known as the likelihood
    					<br/><b> ‚Äì</b> P(A) and P(B) are the probabilities of observing A and B independently of each other, also known as prior probability

				<br>-------------------------------------------------------------------------------------------------
				<br><b>
					<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
						<b>Why Naive Bayes is so ‚ÄòNaive‚Äô ?</b>
					</blockquote>
				</b>
				
				-->The Naive Bayes Classifier assumes that a particular feature in a class is independent of other features due to which it gets its name to be ‚ÄúNaive‚Äù. 
				<br>The presence or absence of a feature does not affect other features in the data.

				<br><br>-------------------------------------------------------------------------------------------------
				
				
				<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
					<b> Step-wise Python implementation of Naive Bayes Classifier algorithm.</b>
				</blockquote>
				
					The first step in our algorithm is to load the dataset. We load the Ford Sentence classification dataset in 
					a panda dataframe. The algorithm and all the helper functions are implemented under class FordSentenceClassifier()
 					
				<br>
				<br>
					After that we perform pre-processing to clean up strings by removing
					punctuations, tokenizing sentences into words and a step to count the frequence of words which appear in each sentence.
				<br>
				<br>
				<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
						<b>1. Understanding the algorithm</b>
				</blockquote>
				<br>
					We need three things: the probability that a sentence belongs to a particular class (log priors), a vacublary of words,
					and word frequency for each class/label (number of times a given word appears in sentence of particular class/label)

					We compute the log class priors by counting how many sentences are in our dataset for each class and dividing by the total number.
					Finally we use the math library to take the log						
				<br>
				<p>
					<pre >						
						<code>
	def tokenize(self, text):
		text = self.clean(text).lower()
		return re.split("\W+", text)
				
	def get_word_counts(self, words):
		word_counts = {}
		for word in words:
			word_counts[word] = word_counts.get(word, 0.0) + 1.0
		return word_counts
						</code>
					</pre>
				<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
						<b>2. High level implementation - training, accuracy and predictions</b>
				</blockquote>
				<br>
					After extracting all the above data, we write another fucntion (predict) to out the class label for new data. We apply Naive Base directly
					here. For example, given a sentence, we need to iterate each word and compute the log of how many times we have seen a word belong to each class divided by total count of all words in that class.
					We calculate this for each class and sum them all up.

					Then we add the log class priors to check which score is bigger for that sentence.
					The largest score will be the predicted class.

					<br/>
					<br/>
					<b> Dealing with Imbalanced Data</b>
					Analysis of imbalanced datasets leads to less accurate predictions unless the datasets are properly balanced after pre-processing
					<img src="images/dataTypeClassify.jpg" alt="" >
				<br/>
				<br/>
					<b> OverSampling Technique</b>
					Oversampling is appropriate when data scientists do not have enough information. One class is abundant, or the majority, and the other is rare, or the minority
				<img src="images/overfitting.jpg" alt="" >
				<br/>
				<img src="images/Oversamplingpredictions.jpg" alt="" >
				<br/>
				<b> UnderSampling Technique</b>
				Undersampling is appropriate when there is plenty of data for an accurate analysis. Typically, scientists randomly delete events in the majority classExternal link:open_in_new to end up with the same number of events as the minority class. 
				<img src="images/Undersampling.jpg" alt="" >
				<br/>
				<img src="images/undersamplingpred.jpg" alt="" >
				<br/>
				<b> SMOTE Technique</b>
				SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.
				<img src="images/Smotesmoothing.jpg" alt="" >
				<br/>

				<b>Near Miss Technique</b>
				It can be grouped under undersampling algorithms and is an efficient way to balance the data. The algorithm does this by looking at the class distribution and randomly eliminating samples from the larger class. When two points belonging to different classes are very close to each other in the distribution, this algorithm eliminates the datapoint of the larger class thereby trying to balance the distribution. 
				<img src="images/nearmisssmoothing.jpg" alt="" >
				<br/>
				<b>Conclusions</b>
				SMOTE is also supposed to produce good results. Just make sure that you are not applying SMOTE before using cross-validation (or partitioning your data into train and validation sets). If you do that, there is a high possibility of overfitting because you are introducing duplicates (or synthetic samples) to the validation data set as well which is absolutely not recommended.
				<br/>
				

				<br>

				
					
				<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
					<b>3. Laplace smoothing effect</b>
				</blockquote>
				<br>
					Something to note, log of 0 is undefined! We can encounter a word that is in the vocublary of a certain category but might 
					not be in another class's vocublary. One way around this is to use Laplace smoothing. We simply add 1 to the numerator, but we also
					add size of the vocublary of the denomniator to balance it
				<br>
				<br><br>
				<code>
					log_w_given = math.log( (self.word_counts[label].get(word, 0.0) + 1 if smoothing else 0) / (self.num_messages[label] + len(self.vocab) if smoothing else 0))
			  	</code>

		   <blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
					<b>4. Calculating probability</b>
			</blockquote>
			<br>
				We can calulate probability of occurence of a word by dividing the num of sentences containing ‚Äòthe‚Äô and total sentences.
				We can calulate conditional probability of a word by dividing the num of sentences for particular class containing ‚Äòthe‚Äô and number of sentences which belong to that class
			<br>
			

			<pre >						
				<code>
		def p(self, word, X):
			n = len(X)
			count = 0
			for label in self.labels:
				count += self.word_counts[label][word]
			return count/n
				
		def conditional_p(self, word):
			conditional_p = {}
			for label in self.labels:        
				n = self.word_counts[label][word]
				total = self.num_messages[label]
				conditional_p[label] = n/total
			return conditional_p
				</code>
			</pre>

			<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
				<b>5. Top 5 words for each class </b>
		</blockquote>
		<br>
			Since we already have the count of words for each label, we can use Counter operator
			to find the top 5 words that belong to each class
		<br>
		<pre >						
			<code>
	def top(self, n):
		top_words = {}
		for label in self.labels:
			top_words[label] = dict(Counter(self.word_counts[label]).most_common(5))
		return top_words
			</code>
		</pre>	
		<img src="images/prediction.jpg" alt="" >
				<br/>		
				
				
				<blockquote class="border-l-4 border-green-500 italic my-8 pl-8 md:pl-12">
					
	
			</p>
<!-- <img src="model1.jpg" height="500" width="500" align="left" alt="" ><br>
<img src="model1classifier.jpg" height="500" width="500" align="left" alt="" ><br>
<img src="model1log.jpg" height="500" width="500" align="left" alt="" ><br>
<img src="model1graph.jpg" height="500" width="500" align="left" alt="" ><br>
<img src="axis.jpg" height="120" width="120" align="left" alt="" ><br> -->

			
			
			
						

			
			<!--/ Post Content-->

		</div>

		<!-- Tags
		<div class="text-base md:text-sm text-gray-500 px-4 py-6">
			Tags: <a href="#" class="text-base md:text-sm text-green-500 no-underline hover:underline">Link</a> . <a href="#" class="text-base md:text-sm text-green-500 no-underline hover:underline">Link</a>
		</div> -->

		<!--Divider-->
		<hr class="border-b-2 border-gray-400 mb-8 mx-4">

		<!--Author-->
		<div class="flex w-full items-center font-sans px-4 py-12">
			<img class="w-10 h-10 rounded-full mr-4" src="images/sakshi.jpg" alt="Avatar of Author">
			<div class="flex-1 px-2">
				<p class="text-base font-bold text-base md:text-xl leading-none mb-2">Sakshi</p>
				<p class="text-gray-600 text-xs md:text-base">Learning never Stops</p>
			</div>
			<!-- <div class="justify-end">
				<button class="bg-transparent border border-gray-500 hover:border-green-500 text-xs text-gray-500 hover:text-green-500 font-bold py-2 px-4 rounded-full">Read More</button>
			</div> -->
		</div>
		<!--/Author-->

		<!--Divider-->
		<hr class="border-b-2 border-gray-400 mb-8 mx-4">

		<!--Next & Prev Links-->
		<div class="font-sans flex justify-between content-center px-4 pb-12">
			<div class="text-left">
				<span class="text-xs md:text-sm font-normal text-gray-600"> Previous Post:</span><br>
				<p><a target="_blank" href="https://machinelearningprinciples.blogspot.com/2022/10/MachineLearningPrinciples.html" class="break-normal text-base md:text-sm text-green-500 font-bold no-underline hover:underline">Efficien Guide to Machine Learning predictive model </a></p>
			</div>
			<!-- <div class="text-right">
				<span class="text-xs md:text-sm font-normal text-gray-600">Next Post &gt;</span><br>
				<p><a href="#" class="break-normal text-base md:text-sm text-green-500 font-bold no-underline hover:underline">Blog title</a></p>
			</div> -->
		</div>


		<!--/Next & Prev Links-->

	</div>
	<!--/container-->

	<footer class="bg-white border-t border-gray-400 shadow">
		<div class="container max-w-4xl mx-auto flex py-8">

			<div class="w-full mx-auto flex flex-wrap">
				<div class="flex w-full md:w-1/2 ">
					<div class="px-8">
						<h3 class="font-bold text-gray-900">About</h3>
						<p class="py-4 text-gray-600 text-sm">
							I‚Äôm a lifelong equestrian, and horses have taught me the importance of trusting your instincts, always looking ahead, and communicating with purpose. 
						</p>
					</div>
				</div>

				<div class="flex w-full md:w-1/2">
					<div class="px-8">
						<h3 class="font-bold text-gray-900">Social</h3>
						<ul class="list-reset items-center text-sm pt-3">
							<li>
								<a target="_blank" class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-1" href="https://www.linkedin.com/in/sakshi-srivastava-aa2a5591/">linkedin</a>
							</li>
							<li>
								<a target="_blank" class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-1" href="https://github.com/sakshi0303/">Github</a>
							</li>
							
						</ul>
					</div>
				</div>
			</div>



		</div>
	</footer>

	<script>
		/* Progress bar */
		//Source: https://alligator.io/js/progress-bar-javascript-css-variables/
		var h = document.documentElement,
			b = document.body,
			st = 'scrollTop',
			sh = 'scrollHeight',
			progress = document.querySelector('#progress'),
			scroll;
		var scrollpos = window.scrollY;
		var header = document.getElementById("header");
		var navcontent = document.getElementById("nav-content");

		document.addEventListener('scroll', function() {

			/*Refresh scroll % width*/
			scroll = (h[st] || b[st]) / ((h[sh] || b[sh]) - h.clientHeight) * 100;
			progress.style.setProperty('--scroll', scroll + '%');

			/*Apply classes for slide in bar*/
			scrollpos = window.scrollY;

			if (scrollpos > 10) {
				header.classList.add("bg-white");
				header.classList.add("shadow");
				navcontent.classList.remove("bg-gray-100");
				navcontent.classList.add("bg-white");
			} else {
				header.classList.remove("bg-white");
				header.classList.remove("shadow");
				navcontent.classList.remove("bg-white");
				navcontent.classList.add("bg-gray-100");

			}

		});


		//Javascript to toggle the menu
		document.getElementById('nav-toggle').onclick = function() {
			document.getElementById("nav-content").classList.toggle("hidden");
		}
	</script>

</body>

</html>
